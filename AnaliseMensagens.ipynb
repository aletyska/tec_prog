{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f29453-24cd-40d0-8804-6b139064f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install transformers\n",
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05934c7-dc7a-4a9a-a602-98fab96b71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aletyska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/aletyska/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Imports Dataset Kaggle\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from transformers import BertTokenizer\n",
    "import emoji\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mariumfaheem666/spam-sms-classification-using-nlp\")+\"/\"+\"Spam_SMS.csv\"\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22fb44f0-60bb-4a1e-b1d9-4e92d02455c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de Classes\n",
    "class Mensagem:\n",
    "    def __init__(self, mensagem_original, classificacao):\n",
    "        self.MensagemOriginal = mensagem_original\n",
    "        self.Classificacao = classificacao\n",
    "\n",
    "        #variável temporária para tratamentos\n",
    "        text = self.MensagemOriginal\n",
    "        text = emoji.demojize(text, language='en')\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = text.encode(\"utf-8\", \"ignore\").decode()  \n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        cleaned_sentences = [s.strip() for s in sentences]\n",
    "        \n",
    "        # Msg Tratada (limpa)\n",
    "        self.MensagemTratada = ' '.join(cleaned_sentences)\n",
    "        \n",
    "        # Msg Tokenizada\n",
    "        self.MensagemTokenizada = word_tokenize(self.MensagemTratada)\n",
    "\n",
    "        # Msg Tokenizada para o Bert\n",
    "        self.MensagemTokenizadaBert = tokenizer.encode_plus(\n",
    "            self.MensagemTratada,\n",
    "            add_special_tokens=True,     # Add [CLS] and [SEP]\n",
    "            padding='max_length',        # Pad to max length (or truncate)\n",
    "            truncation=True,             # Truncate to fit max length\n",
    "            max_length=128,              # You can adjust this\n",
    "            return_tensors='pt'          # Return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        self.QuantidadeCaracteres = len(self.MensagemTratada)\n",
    "        self.QuantidadePalavras = len(self.MensagemTokenizada)\n",
    "\n",
    "        tagged = pos_tag(self.MensagemTokenizada)\n",
    "        verbs = [word for word, tag in tagged if tag.startswith('VB')]\n",
    "        self.QuantidadeVerbos = len(verbs)\n",
    "        nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "        self.QuantidadeSubstantivos = len(nouns)\n",
    "\n",
    "    def ConstroiArvoreSintagmas(self):\n",
    "        # Lógica para construir a árvore de sintagmas\n",
    "        self.ArvoreSintagmas = None\n",
    "\n",
    "    def ExibirDadosMensagem(self):\n",
    "        print('Mensagem Original: '+ self.MensagemOriginal)\n",
    "        print('Classificação: '+ self.Classificacao)\n",
    "        print('-----------------------------------------------------------------------------------------------------')\n",
    "        print('Mensagem Tratada: '+ self.MensagemTratada)\n",
    "        print('Mensagem Tokenizada: '+ '|'.join(self.MensagemTokenizada))\n",
    "        print('Quantidade de Caracteres: '+ str(self.QuantidadeCaracteres))\n",
    "        print('Quantidade de Palavras: '+ str(self.QuantidadePalavras))\n",
    "        print('Quantidade de Verbos: '+ str(self.QuantidadeVerbos))\n",
    "        print('Quantidade de Substantivos: '+ str(self.QuantidadeSubstantivos))\n",
    "        \n",
    "\n",
    "\n",
    "class BaseMensagens:\n",
    "    def __init__(self, listamsgs):\n",
    "        self.BaseMensagens = []\n",
    "        for index, row in listamsgs.iterrows():\n",
    "            self.BaseMensagens.append(Mensagem(row['Message'],row['Class']))\n",
    "\n",
    "    def Exec(self, percent):\n",
    "        from collections import defaultdict\n",
    "        import random\n",
    "        grouped = defaultdict(list)\n",
    "        \n",
    "        for msg in base.BaseMensagens:\n",
    "            grouped[msg.Classificacao].append(msg)\n",
    "        \n",
    "        print('\\n###### Original Dataset ######')\n",
    "        for classificacao, group in grouped.items():\n",
    "            random.shuffle(grouped[classificacao])\n",
    "            print(f\"{classificacao}: {len(group)} messages\")\n",
    "        \n",
    "        train = []\n",
    "        test = []\n",
    "        for group_items in grouped.values():\n",
    "            k = max(1, int(len(group_items) * percent / 100))\n",
    "            train.extend(group_items[:k])  # or use random.sample() if you want it shuffled\n",
    "            test.extend(group_items[k:])\n",
    "\n",
    "        grouped = defaultdict(list)\n",
    "        for msg in train:\n",
    "            grouped[msg.Classificacao].append(msg)\n",
    "        print('\\n###### Train Dataset ######')\n",
    "        for classificacao, group in grouped.items():\n",
    "            print(f\"{classificacao}: {len(group)} messages\")\n",
    "\n",
    "        grouped = defaultdict(list)\n",
    "        for msg in test:\n",
    "            grouped[msg.Classificacao].append(msg)\n",
    "        print('\\n###### Test Dataset ######')\n",
    "        for classificacao, group in grouped.items():\n",
    "            print(f\"{classificacao}: {len(group)} messages\")\n",
    "\n",
    "        print('\\n###### Returning Datasets ######')\n",
    "        return BaseTreinamento(train), BaseTeste(test)   \n",
    "\n",
    "    def MostrarBalanceamento(self):\n",
    "        from collections import defaultdict\n",
    "        grouped = defaultdict(list)\n",
    "        \n",
    "        for msg in base.BaseMensagens:\n",
    "            grouped[msg.Classificacao].append(msg)\n",
    "\n",
    "        print('\\n###### Original Dataset ######')\n",
    "        for classificacao, group in grouped.items():\n",
    "            print(f\"{classificacao}: {len(group)} messages\")    \n",
    "            \n",
    "\n",
    "class BaseTreinamento(BaseMensagens):\n",
    "    def __init__(self, base):\n",
    "        self.BaseMensagens = base\n",
    "\n",
    "    def Exec(self):\n",
    "        # Lógica específica de treinamento\n",
    "        pass\n",
    "\n",
    "class BaseTeste(BaseMensagens):\n",
    "    def __init__(self, base):\n",
    "        self.BaseMensagens = base\n",
    "        self.Acuracidade = 0.0\n",
    "\n",
    "    def Exec(self):\n",
    "        self.predict_model()\n",
    "        self.calcular_acuracidade()\n",
    "\n",
    "    def predict_model(self):\n",
    "        pass\n",
    "\n",
    "    def calcular_acuracidade(self):\n",
    "        pass\n",
    "\n",
    "    def calcular_acuracidade(self):\n",
    "        total_mensagens = len(self.BaseMensagens)\n",
    "        acertos = 0\n",
    "\n",
    "        for mensagem in self.BaseMensagens:\n",
    "            if mensagem.Tipo == self.predict_model(mensagem):\n",
    "                acertos += 1\n",
    "\n",
    "        self.Acuracidade = acertos / total_mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d30d23a-f8d0-4ee2-88f7-a2b4acfde53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5550084-e7e1-4e0f-b61b-ec976dd4a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseMensagens(spam_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f02b7d7b-7a97-4db6-8584-e918b9437d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Original Dataset ######\n",
      "ham: 4827 messages\n",
      "spam: 747 messages\n"
     ]
    }
   ],
   "source": [
    "base.MostrarBalanceamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8bff248-4e09-47b0-9a15-15a7d37626b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Original Dataset ######\n",
      "ham: 4827 messages\n",
      "spam: 747 messages\n",
      "\n",
      "###### Train Dataset ######\n",
      "ham: 3378 messages\n",
      "spam: 522 messages\n",
      "\n",
      "###### Test Dataset ######\n",
      "ham: 1449 messages\n",
      "spam: 225 messages\n",
      "\n",
      "###### Returning Datasets ######\n"
     ]
    }
   ],
   "source": [
    "train, test = base.Exec(percent=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d9d0061c-4f1e-43ad-b5ff-875cfc0d2229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BaseTreinamento at 0x7a02ab137830>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31be764d-2165-45ca-8707-764d798069be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BaseTeste at 0x7a02ab9f9d00>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d79fe8-8cbf-4026-b706-d156a57e6ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad96d42-cb97-4e5e-a69e-3db6e84020da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77436c-2032-4402-b52e-0e30fd430b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96968429-4782-493e-b9e0-9f9cce272c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d23d4-888b-4394-a051-5037b1e4acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef4471-7c8a-4837-b101-995a683c9a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481c711-2144-4b20-a1a0-69c7f8092e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bb96e-5d74-474c-a1df-06eda7514493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a7eec-6efd-4d07-8870-6707f6fd9489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f29453-24cd-40d0-8804-6b139064f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install transformers[torch] \n",
    "#!pip install emoji\n",
    "#!pip install datasets \n",
    "#!pip install scikit-learn \n",
    "#!pip install evaluate\n",
    "#!pip install py-cpuinfo gputil psutil\n",
    "#!pip install svgling\n",
    "#!pip install benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05934c7-dc7a-4a9a-a602-98fab96b71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Dataset Kaggle\n",
    "import kagglehub\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mariumfaheem666/spam-sms-classification-using-nlp\")+\"/\"+\"Spam_SMS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fb44f0-60bb-4a1e-b1d9-4e92d02455c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aletyska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import benepar\n",
    "from nltk import Tree\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "nltk.download('punkt')\n",
    "parser = benepar.Parser(\"benepar_en3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Definição de Classes\n",
    "class Mensagem:\n",
    "    def __init__(self, mensagem_original, classificacao):\n",
    "        self.MensagemOriginal = mensagem_original\n",
    "        self.Classificacao = classificacao\n",
    "        self.ClassificacaoInt = 1 if classificacao == 'spam' else 0\n",
    "\n",
    "        #variável temporária para tratamentos\n",
    "        text = self.MensagemOriginal\n",
    "        text = emoji.demojize(text, language='en')\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = text.encode(\"utf-8\", \"ignore\").decode()  \n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        cleaned_sentences = [s.strip() for s in sentences]\n",
    "        \n",
    "        # Msg Tratada (limpa)\n",
    "        self.MensagemTratada = ' '.join(cleaned_sentences)     \n",
    "        self.MensagemTokenizada = word_tokenize(self.MensagemTratada)\n",
    "        self.QuantidadeCaracteres = len(self.MensagemTratada)\n",
    "        self.QuantidadePalavras = len(self.MensagemTokenizada)\n",
    "\n",
    "        tagged = pos_tag(self.MensagemTokenizada)\n",
    "        verbs = [word for word, tag in tagged if tag.startswith('VB')]\n",
    "        self.QuantidadeVerbos = len(verbs)\n",
    "        nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "        self.QuantidadeSubstantivos = len(nouns)\n",
    "        \n",
    "class BaseMensagens:\n",
    "    def __init__(self, listamsgs):\n",
    "        self.BaseMensagens = []\n",
    "        for index, row in listamsgs.iterrows():\n",
    "            self.BaseMensagens.append(Mensagem(row['Message'],row['Class']))\n",
    "        self.BaseMensagensDataFrame = pd.DataFrame([{'text': p.MensagemTratada, 'target': p.ClassificacaoInt} for p in self.BaseMensagens])\n",
    "\n",
    "    def Exec(self, percent):\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(self.BaseMensagensDataFrame['text'], self.BaseMensagensDataFrame['target'], train_size = percent, shuffle = True)\n",
    "        train_tokens = tokenizer(list(train_X), padding = True, truncation=True)\n",
    "        test_tokens = tokenizer(list(test_X), padding = True, truncation=True)\n",
    "        return BaseTreinamento(train_X, train_tokens, train_Y), BaseTeste(test_X, test_tokens, test_Y)\n",
    "\n",
    "class BaseTreinamento(Dataset):\n",
    "    def __init__(self, X, tokens, Y):\n",
    "        self.text_data = X\n",
    "        self.tokens = tokens\n",
    "        self.labels = list(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample\n",
    "\n",
    "class BaseTeste(Dataset):\n",
    "    def __init__(self, X, tokens, Y):\n",
    "        self.text_data = X\n",
    "        self.tokens = tokens\n",
    "        self.labels = list(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample\n",
    "\n",
    "class BertHandler:\n",
    "    def __init__(self, train, test, p_batch_size = 40, bert_pretrained_model = 'bert-base-cased'):\n",
    "        self.batch_size = p_batch_size\n",
    "        self.train_loader = DataLoader(train, shuffle=True, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test, shuffle=True, batch_size=self.batch_size)\n",
    "        self.bert_model = BertForSequenceClassification.from_pretrained(bert_pretrained_model) # Pre-trained model\n",
    "        self.optimizer = AdamW(self.bert_model.parameters(), lr=1e-5) # Optimization function\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss() # Loss function\n",
    "        \n",
    "    def PrintMachineSpecs(self):\n",
    "        import platform\n",
    "        import psutil\n",
    "        import cpuinfo\n",
    "        import GPUtil\n",
    "        import subprocess\n",
    "        \n",
    "        # Informações do sistema operacional\n",
    "        print(\"===== SISTEMA OPERACIONAL =====\")\n",
    "        print(f\"Sistema: {platform.system()} {platform.release()}\")\n",
    "        print(f\"Arquitetura: {platform.architecture()[0]}\")\n",
    "        print(f\"Nome do computador: {platform.node()}\")\n",
    "        \n",
    "        # Informações da CPU\n",
    "        print(\"\\n===== CPU =====\")\n",
    "        cpu_info = cpuinfo.get_cpu_info()\n",
    "        print(f\"Nome da CPU: {cpu_info['brand_raw']}\")\n",
    "        print(f\"Núcleos físicos: {psutil.cpu_count(logical=False)}\")\n",
    "        print(f\"Núcleos lógicos: {psutil.cpu_count(logical=True)}\")\n",
    "        print(f\"Frequência atual: {psutil.cpu_freq().current / 1000:.2f} GHz\")\n",
    "        \n",
    "        # Informações de memória RAM\n",
    "        print(\"\\n===== MEMÓRIA RAM =====\")\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"Total: {mem.total / (1024 ** 3):.2f} GB\")\n",
    "        result = subprocess.run(\n",
    "            ['powershell.exe', '-Command',\n",
    "             \"Get-CimInstance Win32_PhysicalMemory | Measure-Object -Property Capacity -Sum | % { '{0:N2}' -f ($_.Sum / 1GB) }\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        print(f\"RAM Física Total (via PowerShell): {result.stdout.strip()} GB\")\n",
    "        \n",
    "        # Informações da GPU\n",
    "        print(\"\\n===== GPU =====\")\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                print(f\"Nome: {gpu.name}\")\n",
    "                print(f\"Memória Total: {gpu.memoryTotal} MB\")\n",
    "                print(f\"Driver: {gpu.driver}\")\n",
    "                print(f\"ID: {gpu.id}\")\n",
    "        else:\n",
    "            print(\"Nenhuma GPU dedicada detectada.\")\n",
    "\n",
    "    def RunTrainAndTest(self, num_epochs = 1, p_batch_size = 40):\n",
    "        from datetime import datetime\n",
    "        self.batch_size = p_batch_size\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        self.bert_model.to(device) # Transfer model to GPU if available\n",
    "        print('##############################################################')\n",
    "        print('############### BERT TRAINING AND TEST PROCESS ###############')\n",
    "        print('##############################################################')\n",
    "        print('')\n",
    "        self.PrintMachineSpecs()\n",
    "        print('')\n",
    "        print(\"\\n===== PARAMETERS =====\")\n",
    "        print('Epochs: '+str(num_epochs))\n",
    "        print('Batch Size: '+str(self.batch_size))\n",
    "        print('')\n",
    "        begin = datetime.now()\n",
    "        print('############### BEGIN: '+begin.strftime(\"%d/%m/%Y %H:%M:%S\")+' ###############')\n",
    "        for epoch in range(num_epochs):\n",
    "            print('')\n",
    "            print(\"############### EPOCH: \",(epoch + 1))\n",
    "            # TRAINING BLOCK STARTS\n",
    "            self.bert_model.train()\n",
    "            for i,batch in enumerate(self.train_loader):    \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Setting the gradients to zero\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Passing the data to the model\n",
    "                outputs = self.bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "                \n",
    "                # The logits will be used for measuring the loss\n",
    "                pred = outputs.logits\n",
    "                loss = self.loss_fn(pred, batch['labels'])\n",
    "        \n",
    "                # Calculating the gradient for the loss function\n",
    "                loss.backward()\n",
    "                \n",
    "                # Optimizing the parameters of the bert model\n",
    "                self.optimizer.step()\n",
    "        \n",
    "                # Calculating the running loss for logging purposes\n",
    "                train_batch_loss = loss.item()\n",
    "                train_last_loss = train_batch_loss / self.batch_size\n",
    "        \n",
    "                print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "            # Logging epoch-wise training loss\n",
    "            print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "            # TRAINING BLOCK ENDS \n",
    "            \n",
    "            # TESTING BLOCK STARTS\n",
    "            self.bert_model.eval()\n",
    "            correct = 0\n",
    "            test_pred = []\n",
    "            for i, batch in enumerate(self.test_loader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # We don't need gradients for testing\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "                \n",
    "                # Logits act as predictions\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculating total batch loss using the logits and labels\n",
    "                loss = self.loss_fn(logits, batch['labels'])\n",
    "                test_batch_loss = loss.item()\n",
    "                \n",
    "                # Calculating the mean batch loss\n",
    "                test_last_loss = test_batch_loss / self.batch_size\n",
    "                print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
    "                \n",
    "                # Comparing the predicted target with the labels in the batch\n",
    "                correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "                print(\"Testing accuracy: \",correct/((i + 1) * self.batch_size))\n",
    "            \n",
    "            total_test_samples = len(self.test_loader.dataset)\n",
    "            final_accuracy = correct / total_test_samples\n",
    "            print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)\n",
    "            print(f\"Final Testing Accuracy (Epoch {epoch + 1}): {final_accuracy:.4f}\")\n",
    "            # TESTING BLOCK ENDS\n",
    "        end = datetime.now()\n",
    "        print('############### END: '+begin.strftime(\"%d/%m/%Y %H:%M:%S\")+' ###############')\n",
    "        elapsed_minutes = (end - begin).total_seconds() / 60\n",
    "        print(f\"############### TOTAL ELAPSED TIME: {elapsed_minutes:.2f} minutes ###############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d30d23a-f8d0-4ee2-88f7-a2b4acfde53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5550084-e7e1-4e0f-b61b-ec976dd4a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseMensagens(spam_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81137294-6789-472b-8f50-b1bd943a3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArvoreSintagmatica(sentence: str):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tree = parser.parse(tokens)\n",
    "    print(tree.pretty_print())\n",
    "    return tree\n",
    "\n",
    "def ArvoreSintagmaticaRecursao(tree: Tree, indent: int = 0):\n",
    "    if isinstance(tree, str):\n",
    "        # Leaf node (word)\n",
    "        print('  ' * indent + tree)\n",
    "    else:\n",
    "        # Internal node (e.g., S, NP, VP)\n",
    "        print('  ' * indent + tree.label())\n",
    "        for child in tree:\n",
    "            ArvoreSintagmaticaRecursao(child, indent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37025b44-5c5d-44a3-9aab-e2e58e6aa885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/home/aletyska/anaconda3/envs/tec_prog/lib/python3.12/site-packages/torch/distributions/distribution.py:56: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  TOP                                                                     \n",
      "                                                                   |                                                                       \n",
      "                                                                   S                                                                      \n",
      "  _________________________________________________________________|____________________________________________________________________   \n",
      " |          |               |                                ADJP                                  |        |              |            | \n",
      " |          |               |     ____________________________|____                                |        |              |            |  \n",
      " |          |               |    |    |                           ADJP                             |        |              |            | \n",
      " |          |               |    |    |       _____________________|_____                          |        |              |            |  \n",
      " |          |               |    |    |      |                           PP                        |        |              |            | \n",
      " |          |               |    |    |      |       ____________________|_____                    |        |              |            |  \n",
      " |          PP              |    |    |      |      |    |                     NP                  |        NP             VP           | \n",
      " |     _____|_____          |    |    |      |      |    |          ___________|________           |    ____|____      ____|____        |  \n",
      " |    |           NP        |   ADJP  |      |     ADVP  |         NP                   NP         |   NP       ADVP  |         NP      | \n",
      " |    |      _____|____     |    |    |      |      |    |     ____|___________      ___|____      |   |         |    |     ____|___    |  \n",
      " VB   IN    JJ         NN   ,    JJ   :      JJ     RB   IN   FW   FW    JJ    NN   FW  FW   FW    :   NN        EX  VBD   JJ       FW  : \n",
      " |    |     |          |    |    |    |      |      |    |    |    |     |     |    |   |    |     |   |         |    |    |        |   |  \n",
      " Go until jurong     point  ,  crazy  .. Available only  in bugis  n   great world  la  e  buffet ... Cine     there got amore     wat ...\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"408px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1136.0,408.0\" width=\"1136px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TOP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.8169%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Go</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.40845%\" y1=\"20px\" y2=\"48px\" /><svg width=\"15.493%\" x=\"2.8169%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"31.8182%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">until</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.9091%\" y1=\"20px\" y2=\"48px\" /><svg width=\"68.1818%\" x=\"31.8182%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">jurong</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">point</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.9091%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.5634%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.11268%\" x=\"18.3099%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.3662%\" y1=\"20px\" y2=\"48px\" /><svg width=\"51.4085%\" x=\"20.4225%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJP</text></svg><svg width=\"9.58904%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">crazy</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.79452%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.47945%\" x=\"9.58904%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">..</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.3288%\" y1=\"20px\" y2=\"48px\" /><svg width=\"84.9315%\" x=\"15.0685%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJP</text></svg><svg width=\"17.7419%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Available</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.87097%\" y1=\"20px\" y2=\"48px\" /><svg width=\"82.2581%\" x=\"17.7419%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"11.7647%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADVP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">only</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.88235%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.84314%\" x=\"11.7647%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.6863%\" y1=\"20px\" y2=\"48px\" /><svg width=\"80.3922%\" x=\"19.6078%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"60.9756%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"28%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">bugis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16%\" x=\"28%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">n</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28%\" x=\"44%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">great</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28%\" x=\"72%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">world</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.4878%\" y1=\"20px\" y2=\"48px\" /><svg width=\"39.0244%\" x=\"60.9756%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">la</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"25%\" x=\"25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">e</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">buffet</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.4878%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.8039%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.871%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.5342%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.1268%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.52113%\" x=\"71.831%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">...</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.5915%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.15493%\" x=\"75.3521%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"46.1538%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Cine</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0769%\" y1=\"20px\" y2=\"48px\" /><svg width=\"53.8462%\" x=\"46.1538%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADVP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">EX</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">there</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.0769%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.9296%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.9718%\" x=\"84.507%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">got</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"20px\" y2=\"48px\" /><svg width=\"70.5882%\" x=\"29.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"58.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">amore</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.1667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"41.6667%\" x=\"58.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">wat</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.1667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.7059%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.493%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.52113%\" x=\"96.4789%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">...</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.2394%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('TOP', [Tree('S', [Tree('VB', ['Go']), Tree('PP', [Tree('IN', ['until']), Tree('NP', [Tree('JJ', ['jurong']), Tree('NN', ['point'])])]), Tree(',', [',']), Tree('ADJP', [Tree('ADJP', [Tree('JJ', ['crazy'])]), Tree(':', ['..']), Tree('ADJP', [Tree('JJ', ['Available']), Tree('PP', [Tree('ADVP', [Tree('RB', ['only'])]), Tree('IN', ['in']), Tree('NP', [Tree('NP', [Tree('FW', ['bugis']), Tree('FW', ['n']), Tree('JJ', ['great']), Tree('NN', ['world'])]), Tree('NP', [Tree('FW', ['la']), Tree('FW', ['e']), Tree('FW', ['buffet'])])])])])]), Tree(':', ['...']), Tree('NP', [Tree('NP', [Tree('NN', ['Cine'])]), Tree('ADVP', [Tree('EX', ['there'])])]), Tree('VP', [Tree('VBD', ['got']), Tree('NP', [Tree('JJ', ['amore']), Tree('FW', ['wat'])])]), Tree(':', ['...'])])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArvoreSintagmatica(base.BaseMensagens[0].MensagemOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2c1ec8d-8a80-4b67-b5ab-3dc142a27758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  TOP                                                                     \n",
      "                                                                   |                                                                       \n",
      "                                                                   S                                                                      \n",
      "  _________________________________________________________________|____________________________________________________________________   \n",
      " |          |               |                                ADJP                                  |        |              |            | \n",
      " |          |               |     ____________________________|____                                |        |              |            |  \n",
      " |          |               |    |    |                           ADJP                             |        |              |            | \n",
      " |          |               |    |    |       _____________________|_____                          |        |              |            |  \n",
      " |          |               |    |    |      |                           PP                        |        |              |            | \n",
      " |          |               |    |    |      |       ____________________|_____                    |        |              |            |  \n",
      " |          PP              |    |    |      |      |    |                     NP                  |        NP             VP           | \n",
      " |     _____|_____          |    |    |      |      |    |          ___________|________           |    ____|____      ____|____        |  \n",
      " |    |           NP        |   ADJP  |      |     ADVP  |         NP                   NP         |   NP       ADVP  |         NP      | \n",
      " |    |      _____|____     |    |    |      |      |    |     ____|___________      ___|____      |   |         |    |     ____|___    |  \n",
      " VB   IN    JJ         NN   ,    JJ   :      JJ     RB   IN   FW   FW    JJ    NN   FW  FW   FW    :   NN        EX  VBD   JJ       FW  : \n",
      " |    |     |          |    |    |    |      |      |    |    |    |     |     |    |   |    |     |   |         |    |    |        |   |  \n",
      " Go until jurong     point  ,  crazy  .. Available only  in bugis  n   great world  la  e  buffet ... Cine     there got amore     wat ...\n",
      "\n",
      "None\n",
      "TOP\n",
      "  S\n",
      "    VB\n",
      "      Go\n",
      "    PP\n",
      "      IN\n",
      "        until\n",
      "      NP\n",
      "        JJ\n",
      "          jurong\n",
      "        NN\n",
      "          point\n",
      "    ,\n",
      "      ,\n",
      "    ADJP\n",
      "      ADJP\n",
      "        JJ\n",
      "          crazy\n",
      "      :\n",
      "        ..\n",
      "      ADJP\n",
      "        JJ\n",
      "          Available\n",
      "        PP\n",
      "          ADVP\n",
      "            RB\n",
      "              only\n",
      "          IN\n",
      "            in\n",
      "          NP\n",
      "            NP\n",
      "              FW\n",
      "                bugis\n",
      "              FW\n",
      "                n\n",
      "              JJ\n",
      "                great\n",
      "              NN\n",
      "                world\n",
      "            NP\n",
      "              FW\n",
      "                la\n",
      "              FW\n",
      "                e\n",
      "              FW\n",
      "                buffet\n",
      "    :\n",
      "      ...\n",
      "    NP\n",
      "      NP\n",
      "        NN\n",
      "          Cine\n",
      "      ADVP\n",
      "        EX\n",
      "          there\n",
      "    VP\n",
      "      VBD\n",
      "        got\n",
      "      NP\n",
      "        JJ\n",
      "          amore\n",
      "        FW\n",
      "          wat\n",
      "    :\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "tree = ArvoreSintagmatica(base.BaseMensagens[0].MensagemOriginal)\n",
    "ArvoreSintagmaticaRecursao(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5150381-2b73-495d-a505-b9d894ed271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = base.Exec(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef73fea7-dc72-404d-8a1d-3b0c5ddaad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = BertHandler(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceae454b-e58e-47cb-9ca0-79ef85eda4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############### BERT TRAINING AND TEST PROCESS ###############\n",
      "##############################################################\n",
      "\n",
      "===== SISTEMA OPERACIONAL =====\n",
      "Sistema: Linux 6.6.87.1-microsoft-standard-WSL2\n",
      "Arquitetura: 64bit\n",
      "Nome do computador: GAMING-SERVER\n",
      "\n",
      "===== CPU =====\n",
      "Nome da CPU: AMD Ryzen 7 2700 Eight-Core Processor\n",
      "Núcleos físicos: 8\n",
      "Núcleos lógicos: 16\n",
      "Frequência atual: 3.39 GHz\n",
      "\n",
      "===== MEMÓRIA RAM =====\n",
      "Total: 15.58 GB\n",
      "RAM Física Total (via PowerShell): 32,00 GB\n",
      "\n",
      "===== GPU =====\n",
      "Nome: NVIDIA GeForce RTX 3060\n",
      "Memória Total: 12288.0 MB\n",
      "Driver: 572.83\n",
      "ID: 0\n",
      "\n",
      "\n",
      "===== PARAMETERS =====\n",
      "Epochs: 1\n",
      "Batch Size: 40\n",
      "\n",
      "############### BEGIN: 15/06/2025 15:12:43 ###############\n",
      "\n",
      "############### EPOCH:  1\n",
      "Training batch 1 last loss: 0.01688491404056549\n",
      "Training batch 2 last loss: 0.014468967914581299\n",
      "Training batch 3 last loss: 0.014773236215114593\n",
      "Training batch 4 last loss: 0.014993014931678771\n",
      "Training batch 5 last loss: 0.01405014842748642\n",
      "Training batch 6 last loss: 0.014664655923843384\n",
      "Training batch 7 last loss: 0.015018735826015473\n",
      "Training batch 8 last loss: 0.011704780906438828\n",
      "Training batch 9 last loss: 0.011665035784244538\n",
      "Training batch 10 last loss: 0.011665050685405732\n",
      "Training batch 11 last loss: 0.010240483283996581\n",
      "Training batch 12 last loss: 0.010297392308712006\n",
      "Training batch 13 last loss: 0.012270869314670562\n",
      "Training batch 14 last loss: 0.010985603183507919\n",
      "Training batch 15 last loss: 0.010245820134878158\n",
      "Training batch 16 last loss: 0.009870795905590058\n",
      "Training batch 17 last loss: 0.011053552478551864\n",
      "Training batch 18 last loss: 0.010233080387115479\n",
      "Training batch 19 last loss: 0.007943675667047501\n",
      "Training batch 20 last loss: 0.009296128153800964\n",
      "Training batch 21 last loss: 0.008711548149585724\n",
      "Training batch 22 last loss: 0.00936310663819313\n",
      "Training batch 23 last loss: 0.008249954879283905\n",
      "Training batch 24 last loss: 0.00905790776014328\n",
      "Training batch 25 last loss: 0.007001303136348724\n",
      "Training batch 26 last loss: 0.005536022782325745\n",
      "Training batch 27 last loss: 0.006657253950834274\n",
      "Training batch 28 last loss: 0.006078268960118293\n",
      "Training batch 29 last loss: 0.004916190356016159\n",
      "Training batch 30 last loss: 0.0065109983086586\n",
      "Training batch 31 last loss: 0.0047623921185731884\n",
      "Training batch 32 last loss: 0.005646674707531929\n",
      "Training batch 33 last loss: 0.004668045789003372\n",
      "Training batch 34 last loss: 0.0049381725490093235\n",
      "Training batch 35 last loss: 0.004418628662824631\n",
      "Training batch 36 last loss: 0.0048615463078022\n",
      "Training batch 37 last loss: 0.006287570297718048\n",
      "Training batch 38 last loss: 0.006235710904002189\n",
      "Training batch 39 last loss: 0.004265303164720536\n",
      "Training batch 40 last loss: 0.005978701636195183\n",
      "Training batch 41 last loss: 0.004207633063197136\n",
      "Training batch 42 last loss: 0.004201281443238258\n",
      "Training batch 43 last loss: 0.0035208828747272493\n",
      "Training batch 44 last loss: 0.00321817472577095\n",
      "Training batch 45 last loss: 0.0035454913973808288\n",
      "Training batch 46 last loss: 0.0033408120274543763\n",
      "Training batch 47 last loss: 0.002704525925219059\n",
      "Training batch 48 last loss: 0.004894783347845077\n",
      "Training batch 49 last loss: 0.002677261270582676\n",
      "Training batch 50 last loss: 0.002606402337551117\n",
      "Training batch 51 last loss: 0.0054613430052995685\n",
      "Training batch 52 last loss: 0.002431483194231987\n",
      "Training batch 53 last loss: 0.0021208506077528\n",
      "Training batch 54 last loss: 0.0030601849779486657\n",
      "Training batch 55 last loss: 0.0021211188286542892\n",
      "Training batch 56 last loss: 0.002665051259100437\n",
      "Training batch 57 last loss: 0.002086557075381279\n",
      "Training batch 58 last loss: 0.001790522038936615\n",
      "Training batch 59 last loss: 0.0029794150963425637\n",
      "Training batch 60 last loss: 0.002567712031304836\n",
      "Training batch 61 last loss: 0.0018139945343136788\n",
      "Training batch 62 last loss: 0.0017357982695102691\n",
      "Training batch 63 last loss: 0.0008866367861628532\n",
      "Training batch 64 last loss: 0.004357736930251122\n",
      "Training batch 65 last loss: 0.003134055435657501\n",
      "Training batch 66 last loss: 0.0020737607032060624\n",
      "Training batch 67 last loss: 0.0024302253499627114\n",
      "Training batch 68 last loss: 0.001256248913705349\n",
      "Training batch 69 last loss: 0.0014363829977810383\n",
      "Training batch 70 last loss: 0.0012617014348506928\n",
      "Training batch 71 last loss: 0.002809755876660347\n",
      "Training batch 72 last loss: 0.0016561375930905342\n",
      "Training batch 73 last loss: 0.002104750834405422\n",
      "Training batch 74 last loss: 0.0018852993845939637\n",
      "Training batch 75 last loss: 0.004466984421014786\n",
      "Training batch 76 last loss: 0.0005287934560328722\n",
      "Training batch 77 last loss: 0.002984345331788063\n",
      "Training batch 78 last loss: 0.00502643957734108\n",
      "Training batch 79 last loss: 0.0013308937661349774\n",
      "Training batch 80 last loss: 0.003630644083023071\n",
      "Training batch 81 last loss: 0.004169383645057678\n",
      "Training batch 82 last loss: 0.002997264824807644\n",
      "Training batch 83 last loss: 0.00494421049952507\n",
      "Training batch 84 last loss: 0.0022419024258852004\n",
      "Training batch 85 last loss: 0.004729938507080078\n",
      "Training batch 86 last loss: 0.0028090264648199083\n",
      "Training batch 87 last loss: 0.0039010070264339446\n",
      "Training batch 88 last loss: 0.001570391282439232\n",
      "Training batch 89 last loss: 0.0010171321220695973\n",
      "Training batch 90 last loss: 0.0024748779833316803\n",
      "Training batch 91 last loss: 0.001616801880300045\n",
      "Training batch 92 last loss: 0.001136916410177946\n",
      "Training batch 93 last loss: 0.002470225468277931\n",
      "Training batch 94 last loss: 0.002396477945148945\n",
      "Training batch 95 last loss: 0.0030817274004220963\n",
      "Training batch 96 last loss: 0.0035104908049106596\n",
      "Training batch 97 last loss: 0.0015229605138301848\n",
      "Training batch 98 last loss: 0.0029457762837409975\n",
      "\n",
      "Training epoch 1 loss:  0.0029457762837409975\n",
      "Testing batch 1 loss: 0.0015712205320596695\n",
      "Testing accuracy:  0.975\n",
      "Testing batch 2 loss: 0.002167031727731228\n",
      "Testing accuracy:  0.975\n",
      "Testing batch 3 loss: 0.0017233040183782578\n",
      "Testing accuracy:  0.9833333333333333\n",
      "Testing batch 4 loss: 0.0017512597143650055\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 5 loss: 0.0010513504967093468\n",
      "Testing accuracy:  0.99\n",
      "Testing batch 6 loss: 0.0009757107123732567\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 7 loss: 0.004134434089064598\n",
      "Testing accuracy:  0.9892857142857143\n",
      "Testing batch 8 loss: 0.0013128233142197133\n",
      "Testing accuracy:  0.990625\n",
      "Testing batch 9 loss: 0.0018017755821347237\n",
      "Testing accuracy:  0.9888888888888889\n",
      "Testing batch 10 loss: 0.0011306045576930045\n",
      "Testing accuracy:  0.99\n",
      "Testing batch 11 loss: 0.0005685596726834774\n",
      "Testing accuracy:  0.990909090909091\n",
      "Testing batch 12 loss: 0.0011261041276156903\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 13 loss: 0.000749266892671585\n",
      "Testing accuracy:  0.9923076923076923\n",
      "Testing batch 14 loss: 0.0009311718866229057\n",
      "Testing accuracy:  0.9928571428571429\n",
      "Testing batch 15 loss: 0.0008680429309606552\n",
      "Testing accuracy:  0.9933333333333333\n",
      "Testing batch 16 loss: 0.003464152291417122\n",
      "Testing accuracy:  0.990625\n",
      "Testing batch 17 loss: 0.002175956591963768\n",
      "Testing accuracy:  0.9897058823529412\n",
      "Testing batch 18 loss: 0.0019282191991806031\n",
      "Testing accuracy:  0.9902777777777778\n",
      "Testing batch 19 loss: 0.0009469207376241684\n",
      "Testing accuracy:  0.9907894736842106\n",
      "Testing batch 20 loss: 0.0014580322429537773\n",
      "Testing accuracy:  0.99\n",
      "Testing batch 21 loss: 0.002230866998434067\n",
      "Testing accuracy:  0.9904761904761905\n",
      "Testing batch 22 loss: 0.003969822824001312\n",
      "Testing accuracy:  0.9886363636363636\n",
      "Testing batch 23 loss: 0.0009820546954870224\n",
      "Testing accuracy:  0.9891304347826086\n",
      "Testing batch 24 loss: 0.0021804004907608034\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 25 loss: 0.0009824907407164573\n",
      "Testing accuracy:  0.988\n",
      "Testing batch 26 loss: 0.00392468199133873\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 27 loss: 0.00352766253054142\n",
      "Testing accuracy:  0.987037037037037\n",
      "Testing batch 28 loss: 0.0008246112614870071\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 29 loss: 0.001676018163561821\n",
      "Testing accuracy:  0.9879310344827587\n",
      "Testing batch 30 loss: 0.0011214408092200755\n",
      "Testing accuracy:  0.9883333333333333\n",
      "Testing batch 31 loss: 0.001361555140465498\n",
      "Testing accuracy:  0.9887096774193549\n",
      "Testing batch 32 loss: 0.0015136817470192908\n",
      "Testing accuracy:  0.9890625\n",
      "Testing batch 33 loss: 0.00037156417965888977\n",
      "Testing accuracy:  0.9893939393939394\n",
      "Testing batch 34 loss: 0.0020891936495900155\n",
      "Testing accuracy:  0.9889705882352942\n",
      "Testing batch 35 loss: 0.0005657477770000697\n",
      "Testing accuracy:  0.9892857142857143\n",
      "Testing batch 36 loss: 0.0011294624768197537\n",
      "Testing accuracy:  0.9895833333333334\n",
      "Testing batch 37 loss: 0.000381237780675292\n",
      "Testing accuracy:  0.9898648648648649\n",
      "Testing batch 38 loss: 0.001697908714413643\n",
      "Testing accuracy:  0.9901315789473685\n",
      "Testing batch 39 loss: 0.0016730135306715966\n",
      "Testing accuracy:  0.9903846153846154\n",
      "Testing batch 40 loss: 0.0014962916262447833\n",
      "Testing accuracy:  0.99\n",
      "Testing batch 41 loss: 0.0021088484674692152\n",
      "Testing accuracy:  0.9896341463414634\n",
      "Testing batch 42 loss: 0.0051866564899683\n",
      "Testing accuracy:  0.9851190476190477\n",
      "\n",
      "Testing epoch 1 last loss:  0.0051866564899683\n",
      "Final Testing Accuracy (Epoch 1): 0.9892\n",
      "############### END: 15/06/2025 15:12:43 ###############\n",
      "############### TOTAL ELAPSED TIME: 1.94 minutes ###############\n"
     ]
    }
   ],
   "source": [
    "bert.RunTrainAndTest(num_epochs=1, p_batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ea03f-02a7-45fc-a94d-995f544375c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980752e-2333-4af1-9a22-1c9b2e3054d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869a9ff-b783-4a0c-942f-a573b6772903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c544f2-151f-4fa3-8676-a225348ac3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e9a53-9db4-48a9-a263-232e66543285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fbfbe-0a7f-4dab-978c-8f9b1389f683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0ca71-57e7-4a52-8ad3-ef38ff1e6c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc944d-a961-409d-925b-2a8a114c4a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f15295-577e-4f1b-b502-e7a6759079a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce2991-077f-409f-89d7-4811c0675e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d42089-4526-4030-9ec1-5ce81aa999c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8098d-613b-48c0-ab25-98e5cc8e8090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85f430-c3ed-4136-95b1-2e9a8999a2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475233a-f323-4e9b-9a51-5e2cd2c9bbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba2b93-ae5b-45ef-8b67-f7da5a3d0cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac7246-7467-4588-b830-7e62fb8905ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb699f-feb9-4262-81ad-9b6a18047b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c3498-28b7-4fc0-8bd8-54f01e2e7a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54d0dd-aaff-4f39-b81a-188d7d12e59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e18ad-73b8-4af2-8b5d-eac9e9f45d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f49415-3feb-47c9-9334-796cca791a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f29453-24cd-40d0-8804-6b139064f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install transformers[torch] \n",
    "#!pip install emoji\n",
    "#!pip install datasets \n",
    "#!pip install scikit-learn \n",
    "#!pip install evaluate\n",
    "#!pip install py-cpuinfo gputil psutil\n",
    "#!pip install svgling\n",
    "#!pip install benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05934c7-dc7a-4a9a-a602-98fab96b71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"mariumfaheem666/spam-sms-classification-using-nlp\")+\"/\"+\"Spam_SMS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fb44f0-60bb-4a1e-b1d9-4e92d02455c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aletyska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import benepar\n",
    "from nltk import Tree\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "nltk.download('punkt')\n",
    "parser = benepar.Parser(\"benepar_en3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "class Mensagem:\n",
    "    def __init__(self, mensagem_original, classificacao):\n",
    "        self.MensagemOriginal = mensagem_original\n",
    "        self.Classificacao = classificacao\n",
    "        self.ClassificacaoInt = 1 if classificacao == 'spam' else 0\n",
    "\n",
    "        text = self.MensagemOriginal\n",
    "        text = emoji.demojize(text, language='en')\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = text.encode(\"utf-8\", \"ignore\").decode()  \n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        cleaned_sentences = [s.strip() for s in sentences]\n",
    "        \n",
    "        self.MensagemTratada = ' '.join(cleaned_sentences)     \n",
    "        self.MensagemTokenizada = word_tokenize(self.MensagemTratada)\n",
    "        self.QuantidadeCaracteres = len(self.MensagemTratada)\n",
    "        self.QuantidadePalavras = len(self.MensagemTokenizada)\n",
    "\n",
    "        tagged = pos_tag(self.MensagemTokenizada)\n",
    "        verbs = [word for word, tag in tagged if tag.startswith('VB')]\n",
    "        self.QuantidadeVerbos = len(verbs)\n",
    "        nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "        self.QuantidadeSubstantivos = len(nouns)\n",
    "        \n",
    "class BaseMensagens:\n",
    "    def __init__(self, listamsgs):\n",
    "        self.BaseMensagens = []\n",
    "        for index, row in listamsgs.iterrows():\n",
    "            self.BaseMensagens.append(Mensagem(row['Message'],row['Class']))\n",
    "        self.BaseMensagensDataFrame = pd.DataFrame([{'text': p.MensagemTratada, 'target': p.ClassificacaoInt} for p in self.BaseMensagens])\n",
    "\n",
    "    def Exec(self, percent):\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(self.BaseMensagensDataFrame['text'], self.BaseMensagensDataFrame['target'], train_size = percent, shuffle = True)\n",
    "        train_tokens = tokenizer(list(train_X), padding = True, truncation=True)\n",
    "        test_tokens = tokenizer(list(test_X), padding = True, truncation=True)\n",
    "        return BaseTreinamento(train_X, train_tokens, train_Y), BaseTeste(test_X, test_tokens, test_Y)\n",
    "\n",
    "class BaseTreinamento(Dataset):\n",
    "    def __init__(self, X, tokens, Y):\n",
    "        self.text_data = X\n",
    "        self.tokens = tokens\n",
    "        self.labels = list(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample\n",
    "\n",
    "class BaseTeste(Dataset):\n",
    "    def __init__(self, X, tokens, Y):\n",
    "        self.text_data = X\n",
    "        self.tokens = tokens\n",
    "        self.labels = list(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample\n",
    "\n",
    "class BertHandler:\n",
    "    def __init__(self, train, test, p_batch_size = 40, bert_pretrained_model = 'bert-base-cased'):\n",
    "        self.batch_size = p_batch_size\n",
    "        self.train_loader = DataLoader(train, shuffle=True, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test, shuffle=True, batch_size=self.batch_size)\n",
    "        self.bert_model = BertForSequenceClassification.from_pretrained(bert_pretrained_model) # Pre-trained model\n",
    "        self.optimizer = AdamW(self.bert_model.parameters(), lr=1e-5) # Optimization function\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss() # Loss function\n",
    "        \n",
    "    def PrintMachineSpecs(self):\n",
    "        import platform\n",
    "        import psutil\n",
    "        import cpuinfo\n",
    "        import GPUtil\n",
    "        import subprocess\n",
    "        \n",
    "        print(\"===== SISTEMA OPERACIONAL =====\")\n",
    "        print(f\"Sistema: {platform.system()} {platform.release()}\")\n",
    "        print(f\"Arquitetura: {platform.architecture()[0]}\")\n",
    "        print(f\"Nome do computador: {platform.node()}\")\n",
    "        \n",
    "        print(\"\\n===== CPU =====\")\n",
    "        cpu_info = cpuinfo.get_cpu_info()\n",
    "        print(f\"Nome da CPU: {cpu_info['brand_raw']}\")\n",
    "        print(f\"Núcleos físicos: {psutil.cpu_count(logical=False)}\")\n",
    "        print(f\"Núcleos lógicos: {psutil.cpu_count(logical=True)}\")\n",
    "        print(f\"Frequência atual: {psutil.cpu_freq().current / 1000:.2f} GHz\")\n",
    "        \n",
    "        # Informações de memória RAM\n",
    "        print(\"\\n===== MEMÓRIA RAM =====\")\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"Total: {mem.total / (1024 ** 3):.2f} GB\")\n",
    "        result = subprocess.run(\n",
    "            ['powershell.exe', '-Command',\n",
    "             \"Get-CimInstance Win32_PhysicalMemory | Measure-Object -Property Capacity -Sum | % { '{0:N2}' -f ($_.Sum / 1GB) }\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        print(f\"RAM Física Total (via PowerShell): {result.stdout.strip()} GB\")\n",
    "        \n",
    "        print(\"\\n===== GPU =====\")\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                print(f\"Nome: {gpu.name}\")\n",
    "                print(f\"Memória Total: {gpu.memoryTotal} MB\")\n",
    "                print(f\"Driver: {gpu.driver}\")\n",
    "                print(f\"ID: {gpu.id}\")\n",
    "        else:\n",
    "            print(\"Nenhuma GPU dedicada detectada.\")\n",
    "\n",
    "    def RunTrainAndTest(self, num_epochs = 1, p_batch_size = 40):\n",
    "        from datetime import datetime\n",
    "        self.batch_size = p_batch_size\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        self.bert_model.to(device) \n",
    "        print('##############################################################')\n",
    "        print('############### BERT TRAINING AND TEST PROCESS ###############')\n",
    "        print('##############################################################')\n",
    "        print('')\n",
    "        self.PrintMachineSpecs()\n",
    "        print('')\n",
    "        print(\"\\n===== PARAMETERS =====\")\n",
    "        print('Epochs: '+str(num_epochs))\n",
    "        print('Batch Size: '+str(self.batch_size))\n",
    "        print('')\n",
    "        begin = datetime.now()\n",
    "        print('############### BEGIN: '+begin.strftime(\"%d/%m/%Y %H:%M:%S\")+' ###############')\n",
    "        for epoch in range(num_epochs):\n",
    "            print('')\n",
    "            print(\"############### EPOCH: \",(epoch + 1))\n",
    "            self.bert_model.train()\n",
    "            for i,batch in enumerate(self.train_loader):    \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "                \n",
    "                pred = outputs.logits\n",
    "                loss = self.loss_fn(pred, batch['labels'])\n",
    "        \n",
    "                loss.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "        \n",
    "                train_batch_loss = loss.item()\n",
    "                train_last_loss = train_batch_loss / self.batch_size\n",
    "        \n",
    "                print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "            print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "            \n",
    "            self.bert_model.eval()\n",
    "            correct = 0\n",
    "            test_pred = []\n",
    "            for i, batch in enumerate(self.test_loader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                \n",
    "                loss = self.loss_fn(logits, batch['labels'])\n",
    "                test_batch_loss = loss.item()\n",
    "                \n",
    "                test_last_loss = test_batch_loss / self.batch_size\n",
    "                print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
    "                \n",
    "                correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "                print(\"Testing accuracy: \",correct/((i + 1) * self.batch_size))\n",
    "            \n",
    "            total_test_samples = len(self.test_loader.dataset)\n",
    "            final_accuracy = correct / total_test_samples\n",
    "            print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)\n",
    "            print(f\"Final Testing Accuracy (Epoch {epoch + 1}): {final_accuracy:.4f}\")\n",
    "            # TESTING BLOCK ENDS\n",
    "        end = datetime.now()\n",
    "        print('############### END: '+begin.strftime(\"%d/%m/%Y %H:%M:%S\")+' ###############')\n",
    "        elapsed_minutes = (end - begin).total_seconds() / 60\n",
    "        print(f\"############### TOTAL ELAPSED TIME: {elapsed_minutes:.2f} minutes ###############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d30d23a-f8d0-4ee2-88f7-a2b4acfde53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5550084-e7e1-4e0f-b61b-ec976dd4a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseMensagens(spam_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81137294-6789-472b-8f50-b1bd943a3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArvoreSintagmatica(sentence: str):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tree = parser.parse(tokens)\n",
    "    print(tree.pretty_print())\n",
    "    return tree\n",
    "\n",
    "def ArvoreSintagmaticaRecursao(tree: Tree, indent: int = 0):\n",
    "    if isinstance(tree, str):\n",
    "        print('  ' * indent + tree)\n",
    "    else:\n",
    "        print('  ' * indent + tree.label())\n",
    "        for child in tree:\n",
    "            ArvoreSintagmaticaRecursao(child, indent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37025b44-5c5d-44a3-9aab-e2e58e6aa885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/home/aletyska/anaconda3/envs/tec_prog/lib/python3.12/site-packages/torch/distributions/distribution.py:56: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  TOP                                                                     \n",
      "                                                                   |                                                                       \n",
      "                                                                   S                                                                      \n",
      "  _________________________________________________________________|____________________________________________________________________   \n",
      " |          |               |                                ADJP                                  |        |              |            | \n",
      " |          |               |     ____________________________|____                                |        |              |            |  \n",
      " |          |               |    |    |                           ADJP                             |        |              |            | \n",
      " |          |               |    |    |       _____________________|_____                          |        |              |            |  \n",
      " |          |               |    |    |      |                           PP                        |        |              |            | \n",
      " |          |               |    |    |      |       ____________________|_____                    |        |              |            |  \n",
      " |          PP              |    |    |      |      |    |                     NP                  |        NP             VP           | \n",
      " |     _____|_____          |    |    |      |      |    |          ___________|________           |    ____|____      ____|____        |  \n",
      " |    |           NP        |   ADJP  |      |     ADVP  |         NP                   NP         |   NP       ADVP  |         NP      | \n",
      " |    |      _____|____     |    |    |      |      |    |     ____|___________      ___|____      |   |         |    |     ____|___    |  \n",
      " VB   IN    JJ         NN   ,    JJ   :      JJ     RB   IN   FW   FW    JJ    NN   FW  FW   FW    :   NN        EX  VBD   JJ       FW  : \n",
      " |    |     |          |    |    |    |      |      |    |    |    |     |     |    |   |    |     |   |         |    |    |        |   |  \n",
      " Go until jurong     point  ,  crazy  .. Available only  in bugis  n   great world  la  e  buffet ... Cine     there got amore     wat ...\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"408px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1136.0,408.0\" width=\"1136px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TOP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.8169%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Go</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.40845%\" y1=\"20px\" y2=\"48px\" /><svg width=\"15.493%\" x=\"2.8169%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"31.8182%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">until</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.9091%\" y1=\"20px\" y2=\"48px\" /><svg width=\"68.1818%\" x=\"31.8182%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">jurong</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">point</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.9091%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.5634%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.11268%\" x=\"18.3099%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.3662%\" y1=\"20px\" y2=\"48px\" /><svg width=\"51.4085%\" x=\"20.4225%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJP</text></svg><svg width=\"9.58904%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">crazy</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.79452%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.47945%\" x=\"9.58904%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">..</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.3288%\" y1=\"20px\" y2=\"48px\" /><svg width=\"84.9315%\" x=\"15.0685%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJP</text></svg><svg width=\"17.7419%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Available</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.87097%\" y1=\"20px\" y2=\"48px\" /><svg width=\"82.2581%\" x=\"17.7419%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"11.7647%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADVP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">only</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.88235%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.84314%\" x=\"11.7647%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.6863%\" y1=\"20px\" y2=\"48px\" /><svg width=\"80.3922%\" x=\"19.6078%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"60.9756%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"28%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">bugis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16%\" x=\"28%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">n</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28%\" x=\"44%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">great</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28%\" x=\"72%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">world</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.4878%\" y1=\"20px\" y2=\"48px\" /><svg width=\"39.0244%\" x=\"60.9756%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">la</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"25%\" x=\"25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">e</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">buffet</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.4878%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.8039%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.871%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.5342%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.1268%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.52113%\" x=\"71.831%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">...</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.5915%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.15493%\" x=\"75.3521%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"46.1538%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Cine</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0769%\" y1=\"20px\" y2=\"48px\" /><svg width=\"53.8462%\" x=\"46.1538%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADVP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">EX</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">there</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.0769%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.9296%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.9718%\" x=\"84.507%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">got</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"20px\" y2=\"48px\" /><svg width=\"70.5882%\" x=\"29.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"58.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">amore</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.1667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"41.6667%\" x=\"58.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FW</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">wat</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.1667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.7059%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.493%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.52113%\" x=\"96.4789%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">...</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.2394%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('TOP', [Tree('S', [Tree('VB', ['Go']), Tree('PP', [Tree('IN', ['until']), Tree('NP', [Tree('JJ', ['jurong']), Tree('NN', ['point'])])]), Tree(',', [',']), Tree('ADJP', [Tree('ADJP', [Tree('JJ', ['crazy'])]), Tree(':', ['..']), Tree('ADJP', [Tree('JJ', ['Available']), Tree('PP', [Tree('ADVP', [Tree('RB', ['only'])]), Tree('IN', ['in']), Tree('NP', [Tree('NP', [Tree('FW', ['bugis']), Tree('FW', ['n']), Tree('JJ', ['great']), Tree('NN', ['world'])]), Tree('NP', [Tree('FW', ['la']), Tree('FW', ['e']), Tree('FW', ['buffet'])])])])])]), Tree(':', ['...']), Tree('NP', [Tree('NP', [Tree('NN', ['Cine'])]), Tree('ADVP', [Tree('EX', ['there'])])]), Tree('VP', [Tree('VBD', ['got']), Tree('NP', [Tree('JJ', ['amore']), Tree('FW', ['wat'])])]), Tree(':', ['...'])])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArvoreSintagmatica(base.BaseMensagens[0].MensagemOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c1ec8d-8a80-4b67-b5ab-3dc142a27758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  TOP                                                                     \n",
      "                                                                   |                                                                       \n",
      "                                                                   S                                                                      \n",
      "  _________________________________________________________________|____________________________________________________________________   \n",
      " |          |               |                                ADJP                                  |        |              |            | \n",
      " |          |               |     ____________________________|____                                |        |              |            |  \n",
      " |          |               |    |    |                           ADJP                             |        |              |            | \n",
      " |          |               |    |    |       _____________________|_____                          |        |              |            |  \n",
      " |          |               |    |    |      |                           PP                        |        |              |            | \n",
      " |          |               |    |    |      |       ____________________|_____                    |        |              |            |  \n",
      " |          PP              |    |    |      |      |    |                     NP                  |        NP             VP           | \n",
      " |     _____|_____          |    |    |      |      |    |          ___________|________           |    ____|____      ____|____        |  \n",
      " |    |           NP        |   ADJP  |      |     ADVP  |         NP                   NP         |   NP       ADVP  |         NP      | \n",
      " |    |      _____|____     |    |    |      |      |    |     ____|___________      ___|____      |   |         |    |     ____|___    |  \n",
      " VB   IN    JJ         NN   ,    JJ   :      JJ     RB   IN   FW   FW    JJ    NN   FW  FW   FW    :   NN        EX  VBD   JJ       FW  : \n",
      " |    |     |          |    |    |    |      |      |    |    |    |     |     |    |   |    |     |   |         |    |    |        |   |  \n",
      " Go until jurong     point  ,  crazy  .. Available only  in bugis  n   great world  la  e  buffet ... Cine     there got amore     wat ...\n",
      "\n",
      "None\n",
      "TOP\n",
      "  S\n",
      "    VB\n",
      "      Go\n",
      "    PP\n",
      "      IN\n",
      "        until\n",
      "      NP\n",
      "        JJ\n",
      "          jurong\n",
      "        NN\n",
      "          point\n",
      "    ,\n",
      "      ,\n",
      "    ADJP\n",
      "      ADJP\n",
      "        JJ\n",
      "          crazy\n",
      "      :\n",
      "        ..\n",
      "      ADJP\n",
      "        JJ\n",
      "          Available\n",
      "        PP\n",
      "          ADVP\n",
      "            RB\n",
      "              only\n",
      "          IN\n",
      "            in\n",
      "          NP\n",
      "            NP\n",
      "              FW\n",
      "                bugis\n",
      "              FW\n",
      "                n\n",
      "              JJ\n",
      "                great\n",
      "              NN\n",
      "                world\n",
      "            NP\n",
      "              FW\n",
      "                la\n",
      "              FW\n",
      "                e\n",
      "              FW\n",
      "                buffet\n",
      "    :\n",
      "      ...\n",
      "    NP\n",
      "      NP\n",
      "        NN\n",
      "          Cine\n",
      "      ADVP\n",
      "        EX\n",
      "          there\n",
      "    VP\n",
      "      VBD\n",
      "        got\n",
      "      NP\n",
      "        JJ\n",
      "          amore\n",
      "        FW\n",
      "          wat\n",
      "    :\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "tree = ArvoreSintagmatica(base.BaseMensagens[0].MensagemOriginal)\n",
    "ArvoreSintagmaticaRecursao(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5150381-2b73-495d-a505-b9d894ed271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = base.Exec(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef73fea7-dc72-404d-8a1d-3b0c5ddaad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = BertHandler(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceae454b-e58e-47cb-9ca0-79ef85eda4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############### BERT TRAINING AND TEST PROCESS ###############\n",
      "##############################################################\n",
      "\n",
      "===== SISTEMA OPERACIONAL =====\n",
      "Sistema: Linux 6.6.87.2-microsoft-standard-WSL2\n",
      "Arquitetura: 64bit\n",
      "Nome do computador: GAMING-SERVER\n",
      "\n",
      "===== CPU =====\n",
      "Nome da CPU: AMD Ryzen 7 2700 Eight-Core Processor\n",
      "Núcleos físicos: 8\n",
      "Núcleos lógicos: 16\n",
      "Frequência atual: 3.39 GHz\n",
      "\n",
      "===== MEMÓRIA RAM =====\n",
      "Total: 15.58 GB\n",
      "RAM Física Total (via PowerShell): 32,00 GB\n",
      "\n",
      "===== GPU =====\n",
      "Nome: NVIDIA GeForce RTX 3060\n",
      "Memória Total: 12288.0 MB\n",
      "Driver: 572.83\n",
      "ID: 0\n",
      "\n",
      "\n",
      "===== PARAMETERS =====\n",
      "Epochs: 1\n",
      "Batch Size: 40\n",
      "\n",
      "############### BEGIN: 01/07/2025 19:28:06 ###############\n",
      "\n",
      "############### EPOCH:  1\n",
      "Training batch 1 last loss: 0.014863574504852295\n",
      "Training batch 2 last loss: 0.014550986886024474\n",
      "Training batch 3 last loss: 0.013954272866249085\n",
      "Training batch 4 last loss: 0.014249780774116516\n",
      "Training batch 5 last loss: 0.012076084315776826\n",
      "Training batch 6 last loss: 0.010607985407114029\n",
      "Training batch 7 last loss: 0.011947425454854966\n",
      "Training batch 8 last loss: 0.00939624160528183\n",
      "Training batch 9 last loss: 0.012981174886226654\n",
      "Training batch 10 last loss: 0.009238064289093018\n",
      "Training batch 11 last loss: 0.0075687989592552185\n",
      "Training batch 12 last loss: 0.010532788187265395\n",
      "Training batch 13 last loss: 0.0076440639793872835\n",
      "Training batch 14 last loss: 0.010554108768701553\n",
      "Training batch 15 last loss: 0.009312771260738373\n",
      "Training batch 16 last loss: 0.0074238359928131105\n",
      "Training batch 17 last loss: 0.006981119513511658\n",
      "Training batch 18 last loss: 0.006513051688671112\n",
      "Training batch 19 last loss: 0.006554402410984039\n",
      "Training batch 20 last loss: 0.005445117875933647\n",
      "Training batch 21 last loss: 0.006521262228488922\n",
      "Training batch 22 last loss: 0.004839293658733368\n",
      "Training batch 23 last loss: 0.0050214648246765135\n",
      "Training batch 24 last loss: 0.006561203300952912\n",
      "Training batch 25 last loss: 0.004391364753246307\n",
      "Training batch 26 last loss: 0.0038175534456968306\n",
      "Training batch 27 last loss: 0.004667178168892861\n",
      "Training batch 28 last loss: 0.004055598750710487\n",
      "Training batch 29 last loss: 0.0037597719579935073\n",
      "Training batch 30 last loss: 0.004091236740350723\n",
      "Training batch 31 last loss: 0.0032511867582798006\n",
      "Training batch 32 last loss: 0.004170852154493332\n",
      "Training batch 33 last loss: 0.002597423270344734\n",
      "Training batch 34 last loss: 0.003198380023241043\n",
      "Training batch 35 last loss: 0.0028910359367728235\n",
      "Training batch 36 last loss: 0.0025173673406243325\n",
      "Training batch 37 last loss: 0.00409935861825943\n",
      "Training batch 38 last loss: 0.0028357962146401405\n",
      "Training batch 39 last loss: 0.0023984353989362716\n",
      "Training batch 40 last loss: 0.0037774983793497086\n",
      "Training batch 41 last loss: 0.0030574694275856016\n",
      "Training batch 42 last loss: 0.002898951806128025\n",
      "Training batch 43 last loss: 0.00238894484937191\n",
      "Training batch 44 last loss: 0.0029681047424674036\n",
      "Training batch 45 last loss: 0.0024908637627959252\n",
      "Training batch 46 last loss: 0.002002169005572796\n",
      "Training batch 47 last loss: 0.0015608158893883229\n",
      "Training batch 48 last loss: 0.0033013515174388887\n",
      "Training batch 49 last loss: 0.002477872557938099\n",
      "Training batch 50 last loss: 0.0019948337227106093\n",
      "Training batch 51 last loss: 0.0017838304862380027\n",
      "Training batch 52 last loss: 0.0013573932461440562\n",
      "Training batch 53 last loss: 0.003187526762485504\n",
      "Training batch 54 last loss: 0.002185244858264923\n",
      "Training batch 55 last loss: 0.00117442375048995\n",
      "Training batch 56 last loss: 0.0010755201801657677\n",
      "Training batch 57 last loss: 0.0031308278441429136\n",
      "Training batch 58 last loss: 0.004849234223365783\n",
      "Training batch 59 last loss: 0.0034888435155153276\n",
      "Training batch 60 last loss: 0.002274787612259388\n",
      "Training batch 61 last loss: 0.0015841158106923102\n",
      "Training batch 62 last loss: 0.0014850998297333718\n",
      "Training batch 63 last loss: 0.0014976084232330323\n",
      "Training batch 64 last loss: 0.0015285839326679707\n",
      "Training batch 65 last loss: 0.003368619829416275\n",
      "Training batch 66 last loss: 0.003483295440673828\n",
      "Training batch 67 last loss: 0.0019724367186427115\n",
      "Training batch 68 last loss: 0.0013941958546638488\n",
      "Training batch 69 last loss: 0.0012794917449355126\n",
      "Training batch 70 last loss: 0.0010028660297393798\n",
      "Training batch 71 last loss: 0.0013305000960826875\n",
      "Training batch 72 last loss: 0.0031619109213352203\n",
      "Training batch 73 last loss: 0.0019403528422117233\n",
      "Training batch 74 last loss: 0.0017464630305767058\n",
      "Training batch 75 last loss: 0.0008112808689475059\n",
      "Training batch 76 last loss: 0.001674533635377884\n",
      "Training batch 77 last loss: 0.0010219546034932136\n",
      "Training batch 78 last loss: 0.0007037610746920108\n",
      "Training batch 79 last loss: 0.001559572946280241\n",
      "Training batch 80 last loss: 0.0018437214195728302\n",
      "Training batch 81 last loss: 0.001756330020725727\n",
      "Training batch 82 last loss: 0.0026389509439468384\n",
      "Training batch 83 last loss: 0.0017103474587202073\n",
      "Training batch 84 last loss: 0.0008295591920614243\n",
      "Training batch 85 last loss: 0.0016366010531783104\n",
      "Training batch 86 last loss: 0.0013232814148068429\n",
      "Training batch 87 last loss: 0.0006971481256186962\n",
      "Training batch 88 last loss: 0.0014241090044379233\n",
      "Training batch 89 last loss: 0.0012140313163399696\n",
      "Training batch 90 last loss: 0.003969775885343552\n",
      "Training batch 91 last loss: 0.002565845474600792\n",
      "Training batch 92 last loss: 0.0007073142565786839\n",
      "Training batch 93 last loss: 0.0006675598677247762\n",
      "Training batch 94 last loss: 0.0023587636649608614\n",
      "Training batch 95 last loss: 0.0008963044732809067\n",
      "Training batch 96 last loss: 0.001247064396739006\n",
      "Training batch 97 last loss: 0.0006975182332098484\n",
      "Training batch 98 last loss: 0.0006768076214939356\n",
      "\n",
      "Training epoch 1 loss:  0.0006768076214939356\n",
      "Testing batch 1 loss: 0.004435441642999649\n",
      "Testing accuracy:  0.975\n",
      "Testing batch 2 loss: 0.0010663719847798347\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 3 loss: 0.0006571437232196331\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 4 loss: 0.0013378597795963287\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 5 loss: 0.00184959527105093\n",
      "Testing accuracy:  0.985\n",
      "Testing batch 6 loss: 0.0008470982313156128\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 7 loss: 0.0008602929301559925\n",
      "Testing accuracy:  0.9892857142857143\n",
      "Testing batch 8 loss: 0.0007387868128716946\n",
      "Testing accuracy:  0.990625\n",
      "Testing batch 9 loss: 0.0007555310614407063\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 10 loss: 0.0009603855200111866\n",
      "Testing accuracy:  0.9925\n",
      "Testing batch 11 loss: 0.0007973399013280868\n",
      "Testing accuracy:  0.9931818181818182\n",
      "Testing batch 12 loss: 0.0015100633725523948\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 13 loss: 0.0015210589393973351\n",
      "Testing accuracy:  0.9903846153846154\n",
      "Testing batch 14 loss: 0.0008521717041730881\n",
      "Testing accuracy:  0.9910714285714286\n",
      "Testing batch 15 loss: 0.0014386679045855998\n",
      "Testing accuracy:  0.99\n",
      "Testing batch 16 loss: 0.00349700003862381\n",
      "Testing accuracy:  0.9890625\n",
      "Testing batch 17 loss: 0.000735699012875557\n",
      "Testing accuracy:  0.9897058823529412\n",
      "Testing batch 18 loss: 0.001469213329255581\n",
      "Testing accuracy:  0.9902777777777778\n",
      "Testing batch 19 loss: 0.0022795837372541427\n",
      "Testing accuracy:  0.9894736842105263\n",
      "Testing batch 20 loss: 0.0009859789162874222\n",
      "Testing accuracy:  0.99\n",
      "Testing batch 21 loss: 0.0009287272579967976\n",
      "Testing accuracy:  0.9904761904761905\n",
      "Testing batch 22 loss: 0.0010183254256844521\n",
      "Testing accuracy:  0.990909090909091\n",
      "Testing batch 23 loss: 0.0015320649370551109\n",
      "Testing accuracy:  0.991304347826087\n",
      "Testing batch 24 loss: 0.002247416600584984\n",
      "Testing accuracy:  0.990625\n",
      "Testing batch 25 loss: 0.0012448719702661038\n",
      "Testing accuracy:  0.991\n",
      "Testing batch 26 loss: 0.0008413323201239109\n",
      "Testing accuracy:  0.9913461538461539\n",
      "Testing batch 27 loss: 0.0005814452189952136\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 28 loss: 0.0011391477659344674\n",
      "Testing accuracy:  0.9919642857142857\n",
      "Testing batch 29 loss: 0.0011831465177237987\n",
      "Testing accuracy:  0.9922413793103448\n",
      "Testing batch 30 loss: 0.0018216850236058234\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 31 loss: 0.0004693558905273676\n",
      "Testing accuracy:  0.9919354838709677\n",
      "Testing batch 32 loss: 0.0008184184320271015\n",
      "Testing accuracy:  0.9921875\n",
      "Testing batch 33 loss: 0.0011328542605042458\n",
      "Testing accuracy:  0.9924242424242424\n",
      "Testing batch 34 loss: 0.0010679432190954686\n",
      "Testing accuracy:  0.9926470588235294\n",
      "Testing batch 35 loss: 0.0027265166863799095\n",
      "Testing accuracy:  0.9921428571428571\n",
      "Testing batch 36 loss: 0.0013212336227297783\n",
      "Testing accuracy:  0.9916666666666667\n",
      "Testing batch 37 loss: 0.0016458924859762193\n",
      "Testing accuracy:  0.9912162162162163\n",
      "Testing batch 38 loss: 0.0010603965260088445\n",
      "Testing accuracy:  0.9914473684210526\n",
      "Testing batch 39 loss: 0.0033848214894533156\n",
      "Testing accuracy:  0.9910256410256411\n",
      "Testing batch 40 loss: 0.001015111617743969\n",
      "Testing accuracy:  0.99125\n",
      "Testing batch 41 loss: 0.0012645582668483258\n",
      "Testing accuracy:  0.9908536585365854\n",
      "Testing batch 42 loss: 0.0014377830550074578\n",
      "Testing accuracy:  0.9863095238095239\n",
      "\n",
      "Testing epoch 1 last loss:  0.0014377830550074578\n",
      "Final Testing Accuracy (Epoch 1): 0.9904\n",
      "############### END: 01/07/2025 19:28:06 ###############\n",
      "############### TOTAL ELAPSED TIME: 1.94 minutes ###############\n"
     ]
    }
   ],
   "source": [
    "bert.RunTrainAndTest(num_epochs=1, p_batch_size=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

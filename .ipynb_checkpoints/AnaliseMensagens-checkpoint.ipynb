{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1f29453-24cd-40d0-8804-6b139064f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install transformers[torch] \n",
    "#!pip install emoji\n",
    "#!pip install datasets \n",
    "#!pip install scikit-learn \n",
    "#!pip install evaluate\n",
    "#!pip install py-cpuinfo gputil psutil\n",
    "#!pip install svgling\n",
    "#!pip install benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05934c7-dc7a-4a9a-a602-98fab96b71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Dataset Kaggle\n",
    "import kagglehub\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mariumfaheem666/spam-sms-classification-using-nlp\")+\"/\"+\"Spam_SMS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fb44f0-60bb-4a1e-b1d9-4e92d02455c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aletyska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/aletyska/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "# Definição de Classes\n",
    "class Mensagem:\n",
    "    def __init__(self, mensagem_original, classificacao):\n",
    "        self.MensagemOriginal = mensagem_original\n",
    "        self.Classificacao = classificacao\n",
    "        self.ClassificacaoInt = 1 if classificacao == 'spam' else 0\n",
    "\n",
    "        #variável temporária para tratamentos\n",
    "        text = self.MensagemOriginal\n",
    "        text = emoji.demojize(text, language='en')\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = text.encode(\"utf-8\", \"ignore\").decode()  \n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        cleaned_sentences = [s.strip() for s in sentences]\n",
    "        \n",
    "        # Msg Tratada (limpa)\n",
    "        self.MensagemTratada = ' '.join(cleaned_sentences)\n",
    "        \n",
    "        self.MensagemTokenizada = word_tokenize(self.MensagemTratada)\n",
    "        self.QuantidadeCaracteres = len(self.MensagemTratada)\n",
    "        self.QuantidadePalavras = len(self.MensagemTokenizada)\n",
    "\n",
    "        tagged = pos_tag(self.MensagemTokenizada)\n",
    "        verbs = [word for word, tag in tagged if tag.startswith('VB')]\n",
    "        self.QuantidadeVerbos = len(verbs)\n",
    "        nouns = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "        self.QuantidadeSubstantivos = len(nouns)\n",
    "\n",
    "        \n",
    "class BaseMensagens:\n",
    "    def __init__(self, listamsgs):\n",
    "        self.BaseMensagens = []\n",
    "        for index, row in listamsgs.iterrows():\n",
    "            self.BaseMensagens.append(Mensagem(row['Message'],row['Class']))\n",
    "        self.BaseMensagensDataFrame = pd.DataFrame([{'text': p.MensagemTratada, 'target': p.ClassificacaoInt} for p in self.BaseMensagens])\n",
    "\n",
    "    def Exec(self, percent):\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(self.BaseMensagensDataFrame['text'], self.BaseMensagensDataFrame['target'], train_size = percent, shuffle = True)\n",
    "        train_tokens = tokenizer(list(train_X), padding = True, truncation=True)\n",
    "        test_tokens = tokenizer(list(test_X), padding = True, truncation=True)\n",
    "        return BaseTreinamento(train_X, train_tokens, train_Y), BaseTeste(test_X, test_tokens, test_Y)\n",
    "\n",
    "class BaseTreinamento(Dataset):\n",
    "    def __init__(self, X, tokens, Y):\n",
    "        self.text_data = X\n",
    "        self.tokens = tokens\n",
    "        self.labels = list(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample\n",
    "\n",
    "class BaseTeste(Dataset):\n",
    "    def __init__(self, X, tokens, Y):\n",
    "        self.text_data = X\n",
    "        self.tokens = tokens\n",
    "        self.labels = list(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample\n",
    "\n",
    "class BertHandler:\n",
    "    def __init__(self, train, test, p_batch_size = 40, bert_pretrained_model = 'bert-base-cased'):\n",
    "        self.batch_size = p_batch_size\n",
    "        self.train_loader = DataLoader(train, shuffle=True, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test, shuffle=True, batch_size=self.batch_size)\n",
    "        self.bert_model = BertForSequenceClassification.from_pretrained(bert_pretrained_model) # Pre-trained model\n",
    "        self.optimizer = AdamW(self.bert_model.parameters(), lr=1e-5) # Optimization function\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss() # Loss function\n",
    "        \n",
    "    def PrintMachineSpecs(self):\n",
    "        import platform\n",
    "        import psutil\n",
    "        import cpuinfo\n",
    "        import GPUtil\n",
    "        import subprocess\n",
    "        \n",
    "        # Informações do sistema operacional\n",
    "        print(\"===== SISTEMA OPERACIONAL =====\")\n",
    "        print(f\"Sistema: {platform.system()} {platform.release()}\")\n",
    "        print(f\"Arquitetura: {platform.architecture()[0]}\")\n",
    "        print(f\"Nome do computador: {platform.node()}\")\n",
    "        \n",
    "        # Informações da CPU\n",
    "        print(\"\\n===== CPU =====\")\n",
    "        cpu_info = cpuinfo.get_cpu_info()\n",
    "        print(f\"Nome da CPU: {cpu_info['brand_raw']}\")\n",
    "        print(f\"Núcleos físicos: {psutil.cpu_count(logical=False)}\")\n",
    "        print(f\"Núcleos lógicos: {psutil.cpu_count(logical=True)}\")\n",
    "        print(f\"Frequência atual: {psutil.cpu_freq().current / 1000:.2f} GHz\")\n",
    "        \n",
    "        # Informações de memória RAM\n",
    "        print(\"\\n===== MEMÓRIA RAM =====\")\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"Total: {mem.total / (1024 ** 3):.2f} GB\")\n",
    "        result = subprocess.run(\n",
    "            ['powershell.exe', '-Command',\n",
    "             \"Get-CimInstance Win32_PhysicalMemory | Measure-Object -Property Capacity -Sum | % { '{0:N2}' -f ($_.Sum / 1GB) }\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        print(f\"RAM Física Total (via PowerShell): {result.stdout.strip()} GB\")\n",
    "        \n",
    "        # Informações da GPU\n",
    "        print(\"\\n===== GPU =====\")\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                print(f\"Nome: {gpu.name}\")\n",
    "                print(f\"Memória Total: {gpu.memoryTotal} MB\")\n",
    "                print(f\"Driver: {gpu.driver}\")\n",
    "                print(f\"ID: {gpu.id}\")\n",
    "        else:\n",
    "            print(\"Nenhuma GPU dedicada detectada.\")\n",
    "\n",
    "    def RunTrainAndTest(self, num_epochs = 3):\n",
    "        from datetime import datetime\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        self.bert_model.to(device) # Transfer model to GPU if available\n",
    "        print('##############################################################')\n",
    "        print('############### BERT TRAINING AND TEST PROCESS ###############')\n",
    "        print('##############################################################')\n",
    "        print('')\n",
    "        self.PrintMachineSpecs()\n",
    "        print('')\n",
    "        begin = datetime.now()\n",
    "        print('############### BEGIN: '+begin.strftime(\"%d/%m/%Y %H:%M:%S\")+' ###############')\n",
    "        for epoch in range(num_epochs):\n",
    "            print('')\n",
    "            print(\"############### EPOCH: \",(epoch + 1))\n",
    "            # TRAINING BLOCK STARTS\n",
    "            self.bert_model.train()\n",
    "            for i,batch in enumerate(self.train_loader):    \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Setting the gradients to zero\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Passing the data to the model\n",
    "                outputs = self.bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "                \n",
    "                # The logits will be used for measuring the loss\n",
    "                pred = outputs.logits\n",
    "                loss = self.loss_fn(pred, batch['labels'])\n",
    "        \n",
    "                # Calculating the gradient for the loss function\n",
    "                loss.backward()\n",
    "                \n",
    "                # Optimizing the parameters of the bert model\n",
    "                self.optimizer.step()\n",
    "        \n",
    "                # Calculating the running loss for logging purposes\n",
    "                train_batch_loss = loss.item()\n",
    "                train_last_loss = train_batch_loss / self.batch_size\n",
    "        \n",
    "                print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "            # Logging epoch-wise training loss\n",
    "            print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "            # TRAINING BLOCK ENDS \n",
    "            \n",
    "            # TESTING BLOCK STARTS\n",
    "            self.bert_model.eval()\n",
    "            correct = 0\n",
    "            test_pred = []\n",
    "            for i, batch in enumerate(self.test_loader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # We don't need gradients for testing\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "                \n",
    "                # Logits act as predictions\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculating total batch loss using the logits and labels\n",
    "                loss = self.loss_fn(logits, batch['labels'])\n",
    "                test_batch_loss = loss.item()\n",
    "                \n",
    "                # Calculating the mean batch loss\n",
    "                test_last_loss = test_batch_loss / self.batch_size\n",
    "                print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
    "                \n",
    "                # Comparing the predicted target with the labels in the batch\n",
    "                correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "                print(\"Testing accuracy: \",correct/((i + 1) * self.batch_size))\n",
    "            \n",
    "            total_test_samples = len(self.test_loader.dataset)\n",
    "            final_accuracy = correct / total_test_samples\n",
    "            print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)\n",
    "            print(f\"Final Testing Accuracy (Epoch {epoch + 1}): {final_accuracy:.4f}\")\n",
    "            # TESTING BLOCK ENDS\n",
    "        end = datetime.now()\n",
    "        print('############### END: '+begin.strftime(\"%d/%m/%Y %H:%M:%S\")+' ###############')\n",
    "        elapsed_minutes = (end - begin).total_seconds() / 60\n",
    "        print(f\"############### TOTAL ELAPSED TIME: {elapsed_minutes:.2f} minutes ###############\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d30d23a-f8d0-4ee2-88f7-a2b4acfde53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5550084-e7e1-4e0f-b61b-ec976dd4a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseMensagens(spam_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5150381-2b73-495d-a505-b9d894ed271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = base.Exec(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef73fea7-dc72-404d-8a1d-3b0c5ddaad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = BertHandler(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748f107c-ae37-43b4-a1e3-9875309025fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SISTEMA OPERACIONAL =====\n",
      "Sistema: Linux 6.6.87.1-microsoft-standard-WSL2\n",
      "Arquitetura: 64bit\n",
      "Nome do computador: GAMING-SERVER\n",
      "\n",
      "===== CPU =====\n",
      "Nome da CPU: AMD Ryzen 7 2700 Eight-Core Processor\n",
      "Núcleos físicos: 8\n",
      "Núcleos lógicos: 16\n",
      "Frequência atual: 3.39 GHz\n",
      "\n",
      "===== MEMÓRIA RAM =====\n",
      "Total: 15.58 GB\n",
      "RAM Física Total (via PowerShell): 32,00 GB\n",
      "\n",
      "===== GPU =====\n",
      "Nome: NVIDIA GeForce RTX 3060\n",
      "Memória Total: 12288.0 MB\n",
      "Driver: 572.83\n",
      "ID: 0\n"
     ]
    }
   ],
   "source": [
    "bert.PrintMachineSpecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceae454b-e58e-47cb-9ca0-79ef85eda4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "############### BERT TRAINING AND TEST PROCESS ###############\n",
      "##############################################################\n",
      "\n",
      "===== SISTEMA OPERACIONAL =====\n",
      "Sistema: Linux 6.6.87.1-microsoft-standard-WSL2\n",
      "Arquitetura: 64bit\n",
      "Nome do computador: GAMING-SERVER\n",
      "\n",
      "===== CPU =====\n",
      "Nome da CPU: AMD Ryzen 7 2700 Eight-Core Processor\n",
      "Núcleos físicos: 8\n",
      "Núcleos lógicos: 16\n",
      "Frequência atual: 3.39 GHz\n",
      "\n",
      "===== MEMÓRIA RAM =====\n",
      "Total: 15.58 GB\n",
      "RAM Física Total (via PowerShell): 32,00 GB\n",
      "\n",
      "===== GPU =====\n",
      "Nome: NVIDIA GeForce RTX 3060\n",
      "Memória Total: 12288.0 MB\n",
      "Driver: 572.83\n",
      "ID: 0\n",
      "\n",
      "############### BEGIN: 14/06/2025 17:07:31 ###############\n",
      "\n",
      "############### EPOCH:  1\n",
      "Training batch 1 last loss: 0.024144372344017027\n",
      "Training batch 2 last loss: 0.02541147470474243\n",
      "Training batch 3 last loss: 0.024397845566272735\n",
      "Training batch 4 last loss: 0.02365110218524933\n",
      "Training batch 5 last loss: 0.021578845381736756\n",
      "Training batch 6 last loss: 0.02080507278442383\n",
      "Training batch 7 last loss: 0.021358320116996767\n",
      "Training batch 8 last loss: 0.019674903154373168\n",
      "Training batch 9 last loss: 0.0185578852891922\n",
      "Training batch 10 last loss: 0.018217645585536957\n",
      "Training batch 11 last loss: 0.01704665571451187\n",
      "Training batch 12 last loss: 0.01596328616142273\n",
      "Training batch 13 last loss: 0.01640581786632538\n",
      "Training batch 14 last loss: 0.015306821465492249\n",
      "Training batch 15 last loss: 0.014265190064907073\n",
      "Training batch 16 last loss: 0.013740214705467223\n",
      "Training batch 17 last loss: 0.012240062654018401\n",
      "Training batch 18 last loss: 0.01212286576628685\n",
      "Training batch 19 last loss: 0.012028193473815918\n",
      "Training batch 20 last loss: 0.009936173260211945\n",
      "Training batch 21 last loss: 0.010087241232395173\n",
      "Training batch 22 last loss: 0.008001810312271119\n",
      "Training batch 23 last loss: 0.01350124329328537\n",
      "Training batch 24 last loss: 0.00924578756093979\n",
      "Training batch 25 last loss: 0.0079616978764534\n",
      "Training batch 26 last loss: 0.010041125118732452\n",
      "Training batch 27 last loss: 0.00864936038851738\n",
      "Training batch 28 last loss: 0.005176989361643791\n",
      "Training batch 29 last loss: 0.006582964956760406\n",
      "Training batch 30 last loss: 0.008009137213230133\n",
      "Training batch 31 last loss: 0.0032048311084508898\n",
      "Training batch 32 last loss: 0.006161962077021599\n",
      "Training batch 33 last loss: 0.006251545995473862\n",
      "Training batch 34 last loss: 0.004078563302755356\n",
      "Training batch 35 last loss: 0.004444584995508194\n",
      "Training batch 36 last loss: 0.004067052900791168\n",
      "Training batch 37 last loss: 0.0050217598676681515\n",
      "Training batch 38 last loss: 0.004016063362360001\n",
      "Training batch 39 last loss: 0.005022343248128891\n",
      "Training batch 40 last loss: 0.004842602834105492\n",
      "Training batch 41 last loss: 0.004159174859523773\n",
      "Training batch 42 last loss: 0.0022871216759085657\n",
      "Training batch 43 last loss: 0.0032094337046146395\n",
      "Training batch 44 last loss: 0.002280178293585777\n",
      "Training batch 45 last loss: 0.0025277113541960715\n",
      "Training batch 46 last loss: 0.0034915152937173843\n",
      "Training batch 47 last loss: 0.0036825239658355713\n",
      "Training batch 48 last loss: 0.0020691407844424248\n",
      "Training batch 49 last loss: 0.004879588261246681\n",
      "Training batch 50 last loss: 0.003167412057518959\n",
      "Training batch 51 last loss: 0.0035825949162244797\n",
      "Training batch 52 last loss: 0.002194145694375038\n",
      "Training batch 53 last loss: 0.0034308739006519316\n",
      "Training batch 54 last loss: 0.0018769297748804092\n",
      "Training batch 55 last loss: 0.005256801098585129\n",
      "Training batch 56 last loss: 0.0030935052782297134\n",
      "Training batch 57 last loss: 0.004732992500066757\n",
      "Training batch 58 last loss: 0.001599147357046604\n",
      "Training batch 59 last loss: 0.002308923564851284\n",
      "Training batch 60 last loss: 0.0013059009797871113\n",
      "Training batch 61 last loss: 0.002327081188559532\n",
      "Training batch 62 last loss: 0.0022737052291631698\n",
      "Training batch 63 last loss: 0.003338770940899849\n",
      "Training batch 64 last loss: 0.0016795048490166665\n",
      "Training batch 65 last loss: 0.001268406491726637\n",
      "Training batch 66 last loss: 0.0015121829695999623\n",
      "Training batch 67 last loss: 0.002465594932436943\n",
      "Training batch 68 last loss: 0.00501534640789032\n",
      "Training batch 69 last loss: 0.0010467713698744773\n",
      "Training batch 70 last loss: 0.001008395664393902\n",
      "Training batch 71 last loss: 0.00036742999218404294\n",
      "Training batch 72 last loss: 0.0017762593924999237\n",
      "Training batch 73 last loss: 0.0017660070210695268\n",
      "Training batch 74 last loss: 0.001446732133626938\n",
      "Training batch 75 last loss: 0.001999199576675892\n",
      "Training batch 76 last loss: 0.0005844944156706333\n",
      "Training batch 77 last loss: 0.0034025888890028\n",
      "Training batch 78 last loss: 0.0010853683575987815\n",
      "Training batch 79 last loss: 0.0004693330265581608\n",
      "Training batch 80 last loss: 0.00044703558087348937\n",
      "Training batch 81 last loss: 0.0006838749162852764\n",
      "Training batch 82 last loss: 0.0002901829080656171\n",
      "Training batch 83 last loss: 0.0010318233631551265\n",
      "Training batch 84 last loss: 0.00037321518175303936\n",
      "Training batch 85 last loss: 0.0002885338384658098\n",
      "Training batch 86 last loss: 0.0002396514406427741\n",
      "Training batch 87 last loss: 0.000500462856143713\n",
      "Training batch 88 last loss: 0.00019830300007015467\n",
      "Training batch 89 last loss: 0.00019037893507629634\n",
      "Training batch 90 last loss: 0.003198724240064621\n",
      "Training batch 91 last loss: 0.002936670556664467\n",
      "Training batch 92 last loss: 0.0019748687744140627\n",
      "Training batch 93 last loss: 0.006275685131549835\n",
      "Training batch 94 last loss: 0.0004042171873152256\n",
      "Training batch 95 last loss: 0.0012528237886726857\n",
      "Training batch 96 last loss: 0.001310843415558338\n",
      "Training batch 97 last loss: 0.004207110032439232\n",
      "Training batch 98 last loss: 0.0009036296978592873\n",
      "\n",
      "Training epoch 1 loss:  0.0009036296978592873\n",
      "Testing batch 1 loss: 0.0005848553963005542\n",
      "Testing accuracy:  1.0\n",
      "Testing batch 2 loss: 0.0019336655735969544\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 3 loss: 0.0014393026940524578\n",
      "Testing accuracy:  0.9833333333333333\n",
      "Testing batch 4 loss: 0.0007357854396104812\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 5 loss: 0.0028674641624093057\n",
      "Testing accuracy:  0.985\n",
      "Testing batch 6 loss: 0.0019483815878629685\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 7 loss: 0.0007082038559019565\n",
      "Testing accuracy:  0.9892857142857143\n",
      "Testing batch 8 loss: 0.0015367778949439525\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 9 loss: 0.0011974855326116085\n",
      "Testing accuracy:  0.9888888888888889\n",
      "Testing batch 10 loss: 0.0009103921242058277\n",
      "Testing accuracy:  0.99\n",
      "Testing batch 11 loss: 0.0006802196614444256\n",
      "Testing accuracy:  0.990909090909091\n",
      "Testing batch 12 loss: 0.0018373053520917893\n",
      "Testing accuracy:  0.9895833333333334\n",
      "Testing batch 13 loss: 0.001880384050309658\n",
      "Testing accuracy:  0.9884615384615385\n",
      "Testing batch 14 loss: 0.0012559257447719574\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 15 loss: 0.0005946943070739508\n",
      "Testing accuracy:  0.9883333333333333\n",
      "Testing batch 16 loss: 0.0008518857881426811\n",
      "Testing accuracy:  0.9890625\n",
      "Testing batch 17 loss: 0.001446877233684063\n",
      "Testing accuracy:  0.9882352941176471\n",
      "Testing batch 18 loss: 0.001279945857822895\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 19 loss: 0.0007984879426658154\n",
      "Testing accuracy:  0.9881578947368421\n",
      "Testing batch 20 loss: 0.0008693297393620014\n",
      "Testing accuracy:  0.98875\n",
      "Testing batch 21 loss: 0.0029847785830497743\n",
      "Testing accuracy:  0.986904761904762\n",
      "Testing batch 22 loss: 0.0008194709196686744\n",
      "Testing accuracy:  0.9875\n",
      "Testing batch 23 loss: 0.001430855691432953\n",
      "Testing accuracy:  0.9880434782608696\n",
      "Testing batch 24 loss: 0.0012766065075993538\n",
      "Testing accuracy:  0.9885416666666667\n",
      "Testing batch 25 loss: 0.0014604290947318078\n",
      "Testing accuracy:  0.988\n",
      "Testing batch 26 loss: 0.0009879589080810547\n",
      "Testing accuracy:  0.9884615384615385\n",
      "Testing batch 27 loss: 0.001224114466458559\n",
      "Testing accuracy:  0.9888888888888889\n",
      "Testing batch 28 loss: 0.000878516212105751\n",
      "Testing accuracy:  0.9892857142857143\n",
      "Testing batch 29 loss: 0.0007603119127452373\n",
      "Testing accuracy:  0.9896551724137931\n",
      "Testing batch 30 loss: 0.003944851458072662\n",
      "Testing accuracy:  0.9883333333333333\n",
      "Testing batch 31 loss: 0.0011390862055122852\n",
      "Testing accuracy:  0.9887096774193549\n",
      "Testing batch 32 loss: 0.0019526254385709763\n",
      "Testing accuracy:  0.98828125\n",
      "Testing batch 33 loss: 0.0016023453325033187\n",
      "Testing accuracy:  0.9886363636363636\n",
      "Testing batch 34 loss: 0.0009522040374577046\n",
      "Testing accuracy:  0.9889705882352942\n",
      "Testing batch 35 loss: 0.0007981217466294765\n",
      "Testing accuracy:  0.9892857142857143\n",
      "Testing batch 36 loss: 0.000871296413242817\n",
      "Testing accuracy:  0.9895833333333334\n",
      "Testing batch 37 loss: 0.002023375406861305\n",
      "Testing accuracy:  0.9891891891891892\n",
      "Testing batch 38 loss: 0.0014010804705321788\n",
      "Testing accuracy:  0.9888157894736842\n",
      "Testing batch 39 loss: 0.001750883273780346\n",
      "Testing accuracy:  0.9884615384615385\n",
      "Testing batch 40 loss: 0.0007089123129844666\n",
      "Testing accuracy:  0.98875\n",
      "Testing batch 41 loss: 0.0010759538039565087\n",
      "Testing accuracy:  0.9890243902439024\n",
      "Testing batch 42 loss: 0.0011690638959407807\n",
      "Testing accuracy:  0.9845238095238096\n",
      "\n",
      "Testing epoch 1 last loss:  0.0011690638959407807\n",
      "Final Testing Accuracy (Epoch 1): 0.9886\n",
      "############### END: 14/06/2025 17:07:31 ###############\n",
      "############### TOTAL ELAPSED TIME: 1.60 minutes ###############\n"
     ]
    }
   ],
   "source": [
    "bert.RunTrainAndTest(num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3936ea2-da5a-43e6-821c-6f403541e38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605ca58-9c86-43d4-a358-5469a5032c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94446118-42f8-47d3-ac52-3ab9bf91bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a73f87-708c-4f13-b6bd-42e3a74ed88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af3e9e-c05f-410c-9172-af7d608ab471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6eaa90-c7df-4304-92ee-9381ae5bad4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00000f4-e641-4543-bb85-5b69d8f6a2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658c55a-d8b7-46dc-a4b5-58c49dd5d2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2e25d-0280-475a-bffd-d3d857c1338a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017ae4a-ab7d-4702-8446-f3acd7392303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5edac-a1c2-4364-8fce-622960815b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0b894-f6ca-45ab-9621-cee879fed0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
